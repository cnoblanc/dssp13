{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From LDA, to naive Bayes to Logistic Regression\n",
    "\n",
    "Author: Alexandre Gramfort\n",
    "\n",
    "LDA - QDA: 1h30\n",
    "Naive Bayes, logreg, learning curves (03): 1h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notations of binary classification:\n",
    "\n",
    "- $\\mathcal{Y}$ is the set of labels, here we use $\\mathcal{Y} = \\{-1,1\\}$ in a binary classification setup,\n",
    "- $\\mathbf{x} = (x_1,\\dots,x_p) \\in \\mathcal{X}\\subset \\mathbb{R}^p$ is an observation (a sample),\n",
    "- $ \\mathcal{D}_n = \\{(\\mathbf{x}_i , y_i), i=1,\\dots n\\}$ a train set\n",
    "containing $n$ samples and the associated labels,\n",
    "- there is a probability model which governs the generation of the data $X$ et $Y$:\n",
    "$$ \\forall i \\in \\{1,\\dots,n\\},  (\\mathbf{x}_i , y_i) \\stackrel{i.i.d }{\\sim} (X,Y)$$.\n",
    "- The objective is to construct from a training set $ \\mathcal{D}_n $ a function\n",
    "$\\hat{f}:\\mathcal{X} \\mapsto  \\{-1,1\\}$ which for an unknown sample $\\mathbf{x}$\n",
    "(i.e. not present in the training set) can predict its label : $\\hat{f}(\\mathbf{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian distribution\n",
    "\n",
    "Gaussian density in dimension $p$, $\\mathcal{N}_p(\\mu, \\Sigma)$ is given as :\n",
    "$$\n",
    "f(\\mathbf{x}) = \\frac{1}{(2\\pi)^{p/2} \\sqrt{\\det(\\Sigma)}} \\exp\\left\\{ -\\frac{1}{2} \n",
    "(\\mathbf{x}-\\mu)^\\top \\Sigma^{-1} (\\mathbf{x}-\\mu)\\right\\}~.\n",
    "$$\n",
    "where the covariance matrix of a random vector $X$ is defined as \n",
    "$\\Sigma = \\mathbb{E} \\bigl[ (X-\\mathbb{E}(X)) (X-\\mathbb{E}(X))^\\top\\bigr]$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### in 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(10000)\n",
    "plt.hist(a, density=True, bins=40);\n",
    "t = np.linspace(-5, 5, 100)\n",
    "plt.plot(t, 1. / np.sqrt(2 * np.pi) * np.exp(-t**2 / 2), 'r', linewidth=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = [2, 2]\n",
    "sigma1 = [[1, 0], [0, 1]]\n",
    "sigma2 = [[4, 0], [0, 1]]\n",
    "sigma3 = [[1, .8], [.8, 1]]\n",
    "\n",
    "X = np.random.multivariate_normal(mu, sigma1, size=2000)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 5))\n",
    "\n",
    "for ax, sigma in zip(axes, [sigma1, sigma2, sigma3]):\n",
    "    X = np.random.multivariate_normal(mu, sigma, size=2000)\n",
    "    ax.scatter(X[:, 0], X[:, 1])\n",
    "    ax.axis('equal')\n",
    "    ax.set_xlim([-5, 10])\n",
    "    ax.set_ylim([-5, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative models\n",
    "\n",
    "Conditional probability:\n",
    "\n",
    "$$\n",
    "f_k(x) = \\mathbb{P}\\{x | y = k\\}\n",
    "$$\n",
    "\n",
    "Probability of being in class k:\n",
    "$$\n",
    "\\mathbb{P}\\{y = k\\} = \\pi_k\n",
    "$$\n",
    "\n",
    "Mixture model:\n",
    "$$\n",
    "\\mathbb{P}\\{x\\} = \\sum_{k \\in \\{-1, 1\\}} \\pi_k f_k(x)\n",
    "$$\n",
    "\n",
    "Bayes' rule:\n",
    "$$\n",
    "\\mathbb{P}\\{y=k | x\\} = \\frac{\\mathbb{P}\\{y = k\\} \\mathbb{P}\\{x | y = k\\}}{\\mathbb{P}\\{x\\}} = \\frac{\\pi_k f_k(x)}{\\sum_{k' \\in \\{-1, 1\\}} \\pi_{k'} f_{k'}(x)}\n",
    "$$\n",
    "\n",
    "### LDA (Linear Discriminant Analysis)\n",
    "\n",
    "When using a linear discriminant analysis (LDA) we assume:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\{x | y=1\\} = \\mathcal{N}_p(\\mu_1, \\Sigma)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\{x | y=-1\\} = \\mathcal{N}_p(\\mu_{-1}, \\Sigma)\n",
    "$$\n",
    "\n",
    "i.e. the conditional probability are Gaussian with **same covariance** but **different centers** for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1 = [2, 2]\n",
    "mu2 = [-2, -3]\n",
    "sigma = [[1, 0], [0, 1]]\n",
    "\n",
    "X1 = np.random.multivariate_normal(mu1, sigma, size=2000)\n",
    "X2 = np.random.multivariate_normal(mu2, sigma, size=2000)\n",
    "\n",
    "plt.scatter(X1[:, 0], X1[:, 1], color='b')\n",
    "plt.scatter(X2[:, 0], X2[:, 1], color='g');\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log ratio:\n",
    "\n",
    "$$\n",
    "\\log \\left(\\frac{\\mathbb{P}\\{Y=+1 \\mid X=\\mathbf{x}\\}}{\\mathbb{P}\\{Y=-1 \\mid X=\\mathbf{x}\\}}\\right)\n",
    "= x^T \\Sigma^{-1} (\\mu_{1} - \\mu_{-1}) + \\frac{1}{2} (\\mu_{1}^T \\Sigma^{-1} \\mu_{1} - \\mu_{-1}^T \\Sigma^{-1} \\mu_{-1}) + \\log(\\frac{\\pi_{1}}{\\pi_{-1}})\n",
    "$$\n",
    "\n",
    "Decision function:\n",
    "\n",
    "$$\n",
    "x^T \\Sigma^{-1} (\\mu_{1} - \\mu_{-1}) + \\frac{1}{2} (\\mu_{1}^T \\Sigma^{-1} \\mu_{1} - \\mu_{-1}^T \\Sigma^{-1} \\mu_{-1}) + \\log(\\frac{\\pi_{1}}{\\pi_{-1}}) > 0 \\Rightarrow y = 1\n",
    "$$\n",
    "\n",
    "It is a **linear** function of the features !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "def demo_clf(clf, X, y, proba=False):\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    h = .02  # step size in the mesh\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    if proba:\n",
    "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "    else:\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(1, figsize=(4, 3))\n",
    "    cmap = plt.cm.bwr\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap, clim=[0, 1], alpha=0.5)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=cmap)\n",
    "\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "X = np.concatenate((X1, X2), axis=0)\n",
    "y = np.array([1] * len(X1) + [-1] * len(X2))\n",
    "\n",
    "demo_clf(LinearDiscriminantAnalysis(), X, y, proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.coef_, clf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QDA (Quadratic discriminant analysis)\n",
    "\n",
    "When using a quadratic discriminant analysis (QDA):\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\{x | y=1\\} = \\mathcal{N}_p(\\mu_1, \\Sigma_{1})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\{x | y=-1\\} = \\mathcal{N}_p(\\mu_{-1}, \\Sigma_{-1})\n",
    "$$\n",
    "\n",
    "i.e. different covariances with different centers for each class.\n",
    "\n",
    "as a consequence with have a **quadratic** boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "mu1 = [2, 2]\n",
    "mu2 = [-2, -3]\n",
    "sigma1 = [[1, 0], [0, 1]]\n",
    "sigma2 = [[1, 0.8], [0.8, 1]]\n",
    "\n",
    "X1 = np.random.multivariate_normal(mu1, sigma1, size=2000)\n",
    "X2 = np.random.multivariate_normal(mu2, sigma2, size=2000)\n",
    "X = np.r_[X1, X2]\n",
    "y = np.array([1] * len(X1) + [-1] * len(X2))\n",
    "\n",
    "demo_clf(QuadraticDiscriminantAnalysis(), X, y, proba=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Naive Bayes is also a **generative** model. It however assumes that all the features are independent conditionnaly on $y$.\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\{x | y=k\\} = \\prod_{i=1}^p \\mathbb{P}\\{x_i | y=k\\}\n",
    "$$\n",
    "\n",
    "### Gaussian Naive Bayes\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\{x_i | y=k\\} = \\frac{1}{(2\\pi)^{1/2} \\sigma_i^k} \\exp \\left\\{ -\\frac{\n",
    "(x_i-\\mu_i^k)^2}{2 (\\sigma_i^k)^2}\\right\\}~.\n",
    "$$\n",
    "\n",
    "As the variance parameters dependent on the class we have a quadractic boundary condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "mu1 = [2, 2]\n",
    "mu2 = [-2, -3]\n",
    "sigma1 = [[0.3, -0.2], [-0.2, 1]]\n",
    "sigma2 = [[1, 0.8], [0.8, 1]]\n",
    "\n",
    "X1 = np.random.multivariate_normal(mu1, sigma1, size=2000)\n",
    "X2 = np.random.multivariate_normal(mu2, sigma2, size=2000)\n",
    "X = np.r_[X1, X2]\n",
    "y = np.array([1] * len(X1) + [-1] * len(X2))\n",
    "\n",
    "demo_clf(GaussianNB(), X, y, proba=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question : Can I estimate Naive Bayes with streaming data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic regression is a **discriminative** classification approach (although it's called regression...)\n",
    "\n",
    "It follows the model of LDA with a log ratio that is a linear function of the features:\n",
    "\n",
    "$$\n",
    "\\log \\left(\\frac{\\mathbb{P}\\{Y=+1 \\mid X=\\mathbf{x}\\}}{\\mathbb{P}\\{Y=-1 \\mid X=\\mathbf{x}\\}}\\right)\n",
    "= x^T \\beta + \\beta_0\n",
    "$$\n",
    "\n",
    "Decision function:\n",
    "\n",
    "$$\n",
    "x^T \\beta + \\beta_0 > 0 \\Rightarrow y = 1\n",
    "$$\n",
    "\n",
    "It is a **linear** function of the features !\n",
    "\n",
    "We then can get the conditional probabilities:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\{Y=1 \\mid X=\\mathbf{x}\\} = \\frac{\\exp(\\mathbf{x}^T \\beta + \\beta_0)}{1 + \\exp(\\mathbf{x}^T \\beta + \\beta_0)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\{Y=-1 \\mid X=\\mathbf{x}\\} = \\frac{1}{1 + \\exp(\\mathbf{x}^T \\beta + \\beta_0)}\n",
    "$$\n",
    "\n",
    "In practice $\\beta$ and $\\beta_0$ are computed by maximizing the likelihood of the training data under this model. It reads:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}, \\hat{\\beta}_0 = argmin_{\\beta, \\beta_0} \\sum_{i=1}^n \\sum_k 1_{\\{Y_i = k\\}} \\log (\\mathbb{P}\\{Y=k \\mid X=\\mathbf{x}_i, \\beta, \\beta_0 \\})\n",
    "$$\n",
    "\n",
    "One can show that it leads with y=1 or y=-1 to:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}, \\hat{\\beta}_0 = argmin_{\\beta, \\beta_0} \\sum_{i=1}^n \\log \\{1 + \\exp(-y_i(\\mathbf{x}_i^T \\beta + \\beta_0) \\})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "mu1 = [2, 2]\n",
    "mu2 = [-2, -3]\n",
    "sigma1 = [[0.3, -0.2], [-0.2, 1]]\n",
    "sigma2 = [[1, 0.8], [0.8, 1]]\n",
    "\n",
    "X1 = np.random.multivariate_normal(mu1, sigma1, size=2000)\n",
    "X2 = np.random.multivariate_normal(mu2, sigma2, size=2000)\n",
    "X = np.r_[X1, X2]\n",
    "y = np.array([1] * len(X1) + [-1] * len(X2))\n",
    "\n",
    "demo_clf(LogisticRegression(solver='lbfgs'), X, y, proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def demo_clf(clf, X, y, proba=False):\n",
    "    clf.fit(PolynomialFeatures(2).fit_transform(X), y)\n",
    "\n",
    "    h = .02  # step size in the mesh\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    XX = PolynomialFeatures(2).fit_transform(np.c_[xx.ravel(), yy.ravel()])\n",
    "    if proba:\n",
    "        Z = clf.predict_proba(XX)[:, 1]\n",
    "    else:\n",
    "        Z = clf.predict(XX)\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(1, figsize=(4, 3))\n",
    "    cmap = plt.cm.bwr\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap, clim=[0, 1])\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=cmap)\n",
    "\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "demo_clf(LogisticRegression(solver='lbfgs'), X, y, proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
