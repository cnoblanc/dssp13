{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs for Sentiment Analysis and Text Generation\n",
    "This part of the lab is based on Antoine Tixier's notes [Introduction to CNNs and LSTMs for NLP](https://arxiv.org/pdf/1808.09772.pdf). You are strongly encouraged to have a look at these notes for a quick theoretical intro.\n",
    "\n",
    "## Part 1: Sentiment Classification using LSTMs\n",
    "In the first part of the lab, we will implement a long short-term memory (LSTM) network to perform binary movie review classification (positive/negative) using the [Keras](https://keras.io) library.\n",
    "\n",
    "For our experiments, we will use the [sentence polarity dataset](http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz). The dataset was collected by Pang and Lee and consists of 5,331 positive and 5,331 negative snippets acquired from Rotten Tomatoes. Snippets were automatically labeled using the labels provided by Rotten Tomatoes. The positive and negative reviews are stored into the `rt-polarity.pos` and `rt-polarity.neg` files, respectively. Let's first read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A positive review: the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "\n",
      "\n",
      "A negative review: simplistic , silly and tedious . \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_documents(filename):\n",
    "    docs =[]\n",
    "\n",
    "    with open(filename, encoding='utf8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            docs.append(line[:-1])\n",
    "\n",
    "    return docs\n",
    "\n",
    "docs = list()\n",
    "labels = list()\n",
    "\n",
    "docs_pos = load_documents('data/rt-polarity.pos')\n",
    "docs.extend(docs_pos)\n",
    "labels.extend([1]*len(docs_pos))\n",
    "\n",
    "docs_neg = load_documents('data/rt-polarity.neg')\n",
    "docs.extend(docs_neg)\n",
    "labels.extend([0]*len(docs_neg))\n",
    "\n",
    "y = np.array(labels)\n",
    "\n",
    "print(\"A positive review:\", docs_pos[0])\n",
    "print('\\n')\n",
    "print(\"A negative review:\", docs_neg[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documents that are contained in the dataset have already undergone some preprocessing. Therefore, we will only remove some punctuation marks, diacritics, and non letters, if any. Furthermore, we will represent each document as a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed document: ['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'s\", 'new', 'conan', 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', ',', 'jean', 'claud', 'van', 'damme', 'or', 'steven', 'segal']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().split()\n",
    "\n",
    "    \n",
    "def preprocessing(docs):\n",
    "    preprocessed_docs = []\n",
    "\n",
    "    for doc in docs:\n",
    "        preprocessed_docs.append(clean_str(doc))\n",
    "\n",
    "    return preprocessed_docs\n",
    "\n",
    "processed_docs = preprocessing(docs)\n",
    "\n",
    "print(\"Preprocessed document:\", processed_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, we will extract the vocabulary of the dataset. We will store the vocabulary in a dictionary where keys are terms and values correspond to indices. Hence, each term will be assigned a unique index. The minimum index will be equal to 1, while the maximum index will be equal to the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary: 18777\n",
      "Index of term 'good': 72\n"
     ]
    }
   ],
   "source": [
    "def get_vocab(processed_docs):\n",
    "    vocab = dict()\n",
    "\n",
    "    for doc in processed_docs:\n",
    "        for word in doc:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab) + 1\n",
    "\n",
    "    return vocab\n",
    "\n",
    "vocab = get_vocab(processed_docs)\n",
    "print(\"Size of the vocabulary:\", len(vocab))\n",
    "print(\"Index of term 'good':\", vocab[\"good\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load a set of 300-dimensional word embeddings learned with word2vec on the GoogleNews dataset. The embeddings can be downloaded from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing. Using `gensim`, we can extract only the vectors of the words found in our vocabulary. Terms not present in the set of pre-trained words are initialized randomly (uniformly in [−0.25, 0.25]). Before executing the code, set the path for the file that contains the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "def load_embeddings(fname, vocab):\n",
    "    embeddings = np.zeros((len(vocab)+1, 300))\n",
    "    \n",
    "    model = KeyedVectors.load_word2vec_format(fname, binary=True)\n",
    "    for word in vocab:\n",
    "        if word in model:\n",
    "            embeddings[vocab[word]] = model[word]\n",
    "        else:\n",
    "            embeddings[vocab[word]] = np.random.uniform(-0.25, 0.25, 300)\n",
    "    return embeddings\n",
    "\n",
    "path_to_embeddings = '/Users/christophenoblanc/Documents/ProjetsPython/DSSP/Day_17_et_18/GoogleNews-vectors-negative300.bin.gz'\n",
    "embeddings = load_embeddings(path_to_embeddings, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now calculate the size of the largest document and create a matrix whose rows correspond to documents. Each row contains the indices of the terms appearing in the document and preserves the order of the terms in the document. That is, the first component of a row contains the index of the first term of the corresponding document, the second component contains the index of the second term etc. Documents whose length is shorter than that of the longest document are padded with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#your code here\n",
    "\n",
    "# not done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then use the [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function of scikit-learn to split our dataset randomly into a training and a test set. Set the size of the test set to 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#your code here\n",
    "\n",
    "# not done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the LSTM\n",
    "\n",
    "To build the neural network, we will make use of the Sequential model. We will first add an [Embedding layer](https://keras.io/layers/embeddings/). The Embedding layer requires the input data to be integer encoded, so that each word is represented by a unique integer. The Embedding layer can be initialized either with random weights and learn an embedding for all of the words in the training set or with pre-trained word embeddings. In our case, it will be initialized with the 300-dimensional word embeddings that we have already loaded. The Embedding layer must specify 3 arguments: (1) `input_dim`: the size of the vocabulary, (2) `output_dim`: the size of the vector space in which the words have been embedded (i.e., 300 in our case), and (3) `input_length`: the maximum length of the input documents. In case we initialize the layer with pre-trained embeddings, we must provide another argument (`weights`) which is a list that contains a matrix whose i-th row corresponds to the embedding of term with index i. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "#your code here\n",
    "\n",
    "# not done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then add a [Long Short-Term Memory layer](https://keras.io/layers/recurrent/#lstm). The Long Short-Term Memory layer takes as input the number of hidden units (i.e., dimensionality of the output). Set the number of units to 100. To create a Bidirectional LSTM, we will use the [Bidirectional layer wrapper](https://keras.io/layers/wrappers/#bidirectional). This wrapper takes a recurrent layer as an argument (the Long Short-Term Memory layer in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "\n",
    "#your code here\n",
    "\n",
    "# not done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will apply [dropout](https://keras.io/layers/core/#dropout) to the output of the LSTM (with rate 0.5) and we will add to the model a fully connected layer with one hidden unit whose value corresponds to the probability that the review is positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "#your code here\n",
    "\n",
    "# not done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will compile the model. Since this is a binary classification task, the loss function is the binary crossentropy. To train the network, we will use the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#your code here\n",
    "\n",
    "# not done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally print the details of the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model on CPU. Note you can get a significant speedup by using a GPU. We add a callback which saves the model that has achieved the highest test accuracy to the disk. We also add a second callback which ensures that training stops after 2 epochs without improvement in test set accuracy (early stopping strategy). Use the [fit](https://keras.io/models/model/#methods) function of Keras to train the model. Set the number of epochs to 5 and the batch size to 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', # go through epochs as long as accuracy on validation set increases\n",
    "                               patience=2,\n",
    "                               mode='max')\n",
    "\n",
    "# make sure that the model corresponding to the best epoch is saved\n",
    "checkpointer = ModelCheckpoint(filepath='bi_lstm.hdf5',\n",
    "                               monitor='val_accuracy',\n",
    "                               save_best_only=True,\n",
    "                               verbose=0)\n",
    "\n",
    "\n",
    "#your code here\n",
    "\n",
    "# not done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Text Generation using LSTMs\n",
    "In the second part of the lab, we will implement an LSTM network to learn sequences of characters, and we will use the model to generate new sequences of characters. Recurrent neural networks such as LSTMs can serve as predictive models, but also as generative models. They can identify the patterns in the data and based on these patterns they can then generate novel data.\n",
    "\n",
    "\n",
    "We will train the LSTM network that we will implement on a technical report of a demo application that was developed by our research team. The textual content of the technical report is stored in the `demo_report.txt` file. Use the code given below to read the file and extract the textual content. How long is the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the role of hashtags in guiding users to content of interest is of critical importance. however, onl\n"
     ]
    }
   ],
   "source": [
    "with open('data/demo_report.txt', encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "#your code here\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10397\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will extract the vocabulary of the text (i.e., all the unique characters). We will also create two dictionaries. One that maps each character to a unique integer, and the reverse dictionary that maps each integer to the corresponding character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '#': 4, \"'\": 5, '(': 6, ')': 7, ',': 8, '-': 9, '.': 10, ':': 11, 'a': 12, 'b': 13, 'c': 14, 'd': 15, 'e': 16, 'f': 17, 'g': 18, 'h': 19, 'i': 20, 'j': 21, 'k': 22, 'l': 23, 'm': 24, 'n': 25, 'o': 26, 'p': 27, 'q': 28, 'r': 29, 's': 30, 't': 31, 'u': 32, 'v': 33, 'w': 34, 'x': 35, 'y': 36, 'z': 37, '’': 38}\n",
      "\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '#', 5: \"'\", 6: '(', 7: ')', 8: ',', 9: '-', 10: '.', 11: ':', 12: 'a', 13: 'b', 14: 'c', 15: 'd', 16: 'e', 17: 'f', 18: 'g', 19: 'h', 20: 'i', 21: 'j', 22: 'k', 23: 'l', 24: 'm', 25: 'n', 26: 'o', 27: 'p', 28: 'q', 29: 'r', 30: 's', 31: 't', 32: 'u', 33: 'v', 34: 'w', 35: 'x', 36: 'y', 37: 'z', 38: '’'}\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "#print('Vocabulary:', vocab)\n",
    "\n",
    "char_to_idx = dict()\n",
    "idx_to_char = dict()\n",
    "\n",
    "#your code here\n",
    "for c in vocab:\n",
    "    char_to_idx[c]=len(char_to_idx)\n",
    "    idx_to_char[char_to_idx[c]]=c\n",
    "    \n",
    "print(char_to_idx)\n",
    "print(\"\")\n",
    "print(idx_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will generate our training set. Specifically, we will split the text into subsequences where the length of each subsequence is 40 characters. Note that the length of the subsequences is a hyperparameter. Therefore, we could have set it equal to smaller or larger values. To generate the subsequences, we slide a window along the text one character at a time. The class label of a subsequence corresponds to the character that follows the subsequence's last character in the text. For instance, the class label of the subsequence “for recent tweets that contain all the term” would be the character “s”. Run the following code to generate the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training matrix: (10357, 40, 39)\n",
      "the role of hashtags in guiding users to\n",
      "next is:  \n",
      "he role of hashtags in guiding users to \n",
      "next is: c\n",
      "e role of hashtags in guiding users to c\n",
      "next is: o\n",
      " role of hashtags in guiding users to co\n",
      "next is: n\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "length = 40\n",
    "sentences = list()\n",
    "next_chars = list()\n",
    "for i in range(0, len(text) - length):\n",
    "    sentences.append(text[i: i + length])\n",
    "    next_chars.append(text[i + length])\n",
    "\n",
    "X = np.zeros((len(sentences), length, len(vocab)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(vocab)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for j, char in enumerate(sentence):\n",
    "        X[i, j, char_to_idx[char]] = 1\n",
    "    y[i, char_to_idx[next_chars[i]]] = 1\n",
    "    \n",
    "print(\"Size of training matrix:\", X.shape) \n",
    "# 10357:samples, 40:sequence of 40 carachers, \n",
    "# 39:vector with False, but True where is the next caracter from the 40 sequence\n",
    "print(sentences[0])\n",
    "print(\"next is:\",next_chars[0])\n",
    "print(sentences[1])\n",
    "print(\"next is:\",next_chars[1])\n",
    "print(sentences[2])\n",
    "print(\"next is:\",next_chars[2])\n",
    "print(sentences[3])\n",
    "print(\"next is:\",next_chars[3])\n",
    "print(X[0,12,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define the LSTM architecture. We will again make use of the Sequential model. We will first add a [Long Short-Term Memory layer](https://keras.io/layers/recurrent/#lstm). The Long Short-Term Memory layer takes as input the number of hidden units (i.e., dimensionality of the output) as well as the size of the input. Set the number of units of the LSTM layer to 64. Moreover, since we will add a second LSTM layer right next to the first one, we need to set the `return_sequences` parameter to True so that the layer returns the full sequence and not just the last output in the sequence. Then, we will add another LSTM layer. Set the number of hidden units to 64. The second LSTM layer will be followed by a fully connected layer ([Dense](https://keras.io/layers/core/#dense)) with as many units as the size of our vocabulary. Since this is a multiclass classification task (i.e., each character corresponds to a class), we will make use of the [softmax](https://keras.io/activations/#softmax) activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "\n",
    "model =Sequential()\n",
    "model.add(LSTM(units=64,return_sequences=True,input_shape=(40,len(vocab))))\n",
    "model.add(LSTM(units=64)) # return_sequences=False : to return the vector of the 40 caracters\n",
    "model.add(Dense(units=len(vocab),activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will compile the model. Since this is a multiclass classification task, the loss function is the categorical crossentropy. To train the network, we will use the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "optimizer = Adam(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next print the details of the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 40, 64)            26624     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 62,183\n",
      "Trainable params: 62,183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model. Use the [fit](https://keras.io/models/model/#methods) function of Keras to train the model. Set the number of epochs to 50 and the batch size to 128. We also add a callback which at the end of each epoch uses the model in order to generate a sequence of characters. Specifically, we randomly sample a subsequence of 40 characters from the text and feed it to the model to generate the next character. We then update the subsequence by removing its first character and adding the predicted character to it. We repeat this process for 100 iterations (i.e., we generate 100 characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10357 samples\n",
      "Epoch 1/50\n",
      "10240/10357 [============================>.] - ETA: 0s - loss: 2.9105Generating text with seed: \"ake subsequent searches for tweets diffi\"\n",
      "ake subsequent searches for tweets diffite te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te t\n",
      "10357/10357 [==============================] - 34s 3ms/sample - loss: 2.9090\n",
      "Epoch 2/50\n",
      "10240/10357 [============================>.] - ETA: 0s - loss: 2.4951Generating text with seed: \"ghquality hashtags. furthermore, it is c\"\n",
      "ghquality hashtags. furthermore, it is cont the the the the the the the the the the the the the the the the the the the the the the the the \n",
      "10357/10357 [==============================] - 27s 3ms/sample - loss: 2.4916\n",
      "Epoch 3/50\n",
      "10240/10357 [============================>.] - ETA: 0s - loss: 2.1740Generating text with seed: \"e recently proposed word mover's distanc\"\n",
      "e recently proposed word mover's distance the the the the the the the the the the the the the the the the the the the the the the the the th\n",
      "10357/10357 [==============================] - 27s 3ms/sample - loss: 2.1729\n",
      "Epoch 4/50\n",
      "10240/10357 [============================>.] - ETA: 0s - loss: 1.9627Generating text with seed: \" set of tweets from which we extract the\"\n",
      " set of tweets from which we extract the tweets the tweets the tweets the tweets the tweets the tweets the tweets the tweets the tweets the \n",
      "10357/10357 [==============================] - 27s 3ms/sample - loss: 1.9598\n",
      "Epoch 5/50\n",
      "10240/10357 [============================>.] - ETA: 0s - loss: 1.7987Generating text with seed: \" return the one the user created. as reg\"\n",
      " return the one the user created. as regrecommected the tweets the tweets the tweets the tweets the tweets the tweets the tweets the tweets \n",
      "10357/10357 [==============================] - 27s 3ms/sample - loss: 1.7979\n",
      "Epoch 6/50\n",
      "10240/10357 [============================>.] - ETA: 0s - loss: 1.6669Generating text with seed: \"ort in order to find appropriate hashtag\"\n",
      "ort in order to find appropriate hashtags the tweets the tweets the tweets the tweets the tweets the tweets the tweets the tweets the tweets\n",
      "10357/10357 [==============================] - 26s 3ms/sample - loss: 1.6676\n",
      "Epoch 7/50\n",
      "10240/10357 [============================>.] - ETA: 0s - loss: 1.5570Generating text with seed: \" same with other tweets. besides duplica\"\n",
      " same with other tweets. besides duplicalled the search the search the search the search the search the search the search the search the sea\n",
      "10357/10357 [==============================] - 27s 3ms/sample - loss: 1.5575\n",
      "Epoch 8/50\n",
      "10240/10357 [============================>.] - ETA: 0s - loss: 1.4655Generating text with seed: \"mover's distance. finally, we capitalize\"\n",
      "mover's distance. finally, we capitalize on the tweets as the tweets as the tweets as the tweets as the tweets as the tweets as the tweets a\n",
      "10357/10357 [==============================] - 27s 3ms/sample - loss: 1.4661\n",
      "Epoch 9/50\n",
      "10240/10357 [============================>.] - ETA: 0s - loss: 1.3869Generating text with seed: \"le, consider the following tweet: \"berni\"\n",
      "le, consider the following tweet: \"berning of the tweet and the tweet and the tweet and the tweet and the tweet and the tweet and the tweet \n",
      "10357/10357 [==============================] - 27s 3ms/sample - loss: 1.3864\n",
      "Epoch 10/50\n",
      "10240/10357 [============================>.] - ETA: 0s - loss: 1.3187Generating text with seed: \"ists of n terms, we create a query that \"\n",
      "ists of n terms, we create a query that as the search apised the search apised the search apised the search apised the search apised the sea\n",
      "10357/10357 [==============================] - 27s 3ms/sample - loss: 1.3185\n",
      "Epoch 11/50\n",
      "10240/10357 [============================>.] - ETA: 0s - loss: 1.2559Generating text with seed: \"ample, consider the following tweet: \"be\"\n",
      "ample, consider the following tweet: \"bersed the similarity the search apised the tweets are the tweets are the tweets are the tweets are th\n",
      "10357/10357 [==============================] - 27s 3ms/sample - loss: 1.2563\n",
      "Epoch 12/50\n",
      "10240/10357 [============================>.] - ETA: 0s - loss: 1.1914Generating text with seed: \"ed tweets, we represent each tweet as a \"\n",
      "ed tweets, we represent each tweet as a tweets apintance between the tweets apintance between the tweets apintance between the tweets apinta\n",
      "10357/10357 [==============================] - 28s 3ms/sample - loss: 1.1891\n",
      "Epoch 13/50\n",
      "10240/10357 [============================>.] - ETA: 0s - loss: 1.1466Generating text with seed: \" by twitter is not totally reliable. the\"\n",
      " by twitter is not totally reliable. the tweets that the tweets that the tweets that the tweets that the tweets that the tweets that the twe\n",
      "10357/10357 [==============================] - 28s 3ms/sample - loss: 1.1454\n",
      "Epoch 14/50\n",
      "10240/10357 [============================>.] - ETA: 0s - loss: 1.0915Generating text with seed: \"ies range from very specific to very gen\"\n",
      "ies range from very specific to very general the tweets contained the user to collect than recommendation set of the tweets contained the us\n",
      "10357/10357 [==============================] - 26s 3ms/sample - loss: 1.0925\n",
      "Epoch 15/50\n",
      "10240/10357 [============================>.] - ETA: 0s - loss: 1.0458Generating text with seed: \"series of queries are generated based on\"\n",
      "series of queries are generated based on the similarity between tweets as of the tweets are recompendows as input tweets as of the tweets ar\n",
      "10357/10357 [==============================] - 27s 3ms/sample - loss: 1.0493\n",
      "Epoch 16/50\n",
      "10240/10357 [============================>.] - ETA: 0s - loss: 0.9994Generating text with seed: \"mmending highquality hashtags. furthermo\"\n",
      "mmending highquality hashtags. furthermore, the similarity between tweets containing tweet. the tweet entered by the user. the tweet entered\n",
      "10357/10357 [==============================] - 26s 3ms/sample - loss: 0.9999\n",
      "Epoch 17/50\n",
      "10240/10357 [============================>.] - ETA: 0s - loss: 0.9572Generating text with seed: \"sed approach with a method based on the \"\n",
      "sed approach with a method based on the tweets compared the tweets compared the tweets compared the tweets compared the tweets compared the \n",
      "10357/10357 [==============================] - 26s 3ms/sample - loss: 0.9585\n",
      "Epoch 18/50\n",
      "10240/10357 [============================>.] - ETA: 0s - loss: 0.9185Generating text with seed: \" hashtags. furthermore, it is clear that\"\n",
      " hashtags. furthermore, it is clear that the tweet are considerable one of the tweet are considerable one of the tweet are considerable one \n",
      "10357/10357 [==============================] - 28s 3ms/sample - loss: 0.9205\n",
      "Epoch 19/50\n",
      "10240/10357 [============================>.] - ETA: 0s - loss: 0.8815Generating text with seed: \"ote some effort in order to find appropr\"\n",
      "ote some effort in order to find approprisent only the user. the user. the user. the user. the user. the user. the user. the user. the user.\n",
      "10357/10357 [==============================] - 29s 3ms/sample - loss: 0.8812\n",
      "Epoch 20/50\n",
      "10240/10357 [============================>.] - ETA: 0s - loss: 0.8542Generating text with seed: \"critical importance. however, only a sma\"\n",
      "critical importance. however, only a smard approprised approprised by the search api in the tweets content is the tweets content is the twee\n",
      "10357/10357 [==============================] - 28s 3ms/sample - loss: 0.8531\n",
      "Epoch 21/50\n",
      "10240/10357 [============================>.] - ETA: 0s - loss: 0.8284Generating text with seed: \"ed to the tweet entered by the user. thi\"\n",
      "ed to the tweet entered by the user. this are relevant to the user and the system the system the system the system the system the system the\n",
      "10357/10357 [==============================] - 28s 3ms/sample - loss: 0.8272\n",
      "Epoch 22/50\n",
      "10240/10357 [============================>.] - ETA: 0s - loss: 0.7878Generating text with seed: \" a considerable number of users and has \"\n",
      " a considerable number of users and has man our to collected tweets that are collected tweets that are collected tweets that are collected t\n",
      "10357/10357 [==============================] - 28s 3ms/sample - loss: 0.7893\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10240/10357 [============================>.] - ETA: 0s - loss: 0.7670Generating text with seed: \" the system has returned ten hashtags as\"\n",
      " the system has returned ten hashtags as that the tweets containing the tweets containing the tweets containing the tweets containing the tw\n",
      "10357/10357 [==============================] - 27s 3ms/sample - loss: 0.7669\n",
      "Epoch 24/50\n",
      "10240/10357 [============================>.] - ETA: 0s - loss: 0.7312Generating text with seed: \" proposed system by computing precision \"\n",
      " proposed system by computing precision and the user.\n",
      "\n",
      "to metween the tweets are proposed as suggestions of the tweets are proposed as sugge\n",
      "10357/10357 [==============================] - 28s 3ms/sample - loss: 0.7319\n",
      "Epoch 25/50\n",
      "10240/10357 [============================>.] - ETA: 0s - loss: 0.7103Generating text with seed: \"stant#tag.\n",
      "\n",
      "given a tweet entered by the\"\n",
      "stant#tag.\n",
      "\n",
      "given a tweet entered by the user. the search api in order to create their tweets are not apploye the search api in order to cre\n",
      "10357/10357 [==============================] - 28s 3ms/sample - loss: 0.7094\n",
      "Epoch 26/50\n",
      " 8448/10357 [=======================>......] - ETA: 3s - loss: 0.6665Generating text with seed: \"he content of the tweet. an overview of \"\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "\n",
    "def generate_text(epoch, _):\n",
    "    # Prints generated text\n",
    "    start_idx = randint(0, len(text) - length - 1)\n",
    "    generated_text = ''\n",
    "    sample_text = text[start_idx: start_idx + length]\n",
    "    generated_text += sample_text\n",
    "    print('Generating text with seed: \"' + sample_text + '\"')\n",
    "\n",
    "    for i in range(100):\n",
    "        X_test = np.zeros((1, length, len(vocab)))\n",
    "        for j, char in enumerate(sample_text):\n",
    "            X_test[0, j, char_to_idx[char]] = 1.\n",
    "\n",
    "        y_pred = model.predict(X_test, verbose=0)[0]\n",
    "        next_idx = np.argmax(y_pred)\n",
    "        next_char = idx_to_char[next_idx]\n",
    "\n",
    "        generated_text += next_char\n",
    "        sample_text = sample_text[1:] + next_char\n",
    "        \n",
    "    print(generated_text)\n",
    "        \n",
    "test_callback = LambdaCallback(on_epoch_end=generate_text)\n",
    "\n",
    "#your code here\n",
    "model.fit(X,y,epochs=50,batch_size=128,callbacks=[test_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
